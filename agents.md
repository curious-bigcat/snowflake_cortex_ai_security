
#Deployment and configuration of Cortex AI GenAI security and guardrails.

### Phase 1: Input Pre-Processing & Detection

1. Use protectai/deberta-v3-base-prompt-injection huggingface model deployed in **Snowpark Container Services (SPCS)** in CORTEX_AI_SECURITY database in public schema for prompt injection check.
2. **Classification Scan**: Run the input through the `AI_FILTER` or a custom Snowpark function to classify the text as "benign" or "malicious".
3. **PII Redaction**: Use `AI_REDACT` to strip out sensitive personal information before it enters the prompt, reducing the impact of a potential leak.

### Phase 2: Secure Prompt Engineering (The "Sandwich" Framework)
Structure your Cortex calls to isolate untrusted user data from privileged system instructions.
1. **Isolate User Data with Delimiters**: Wrap the user's input in XML tags or unique string delimiters (e.g., `###USER_INPUT###`) so the model knows where instructions end and data begins.
2. **Instruction Hierarchy**: Place the most critical safety instructions at the very end of the prompt ("Post-Prompting") to ensure they remain fresh in the model's attention window.
3. **Use the Array-of-Objects Format**: When using `AI_COMPLETE`, separate the `system` and `user` roles. This explicitly defines the "System" persona as the authority, which LLMs are trained to prioritize.

### Phase 3: Native Snowflake Guardrails
Leverage Snowflakeâ€™s built-in managed features for the final output stage.
1. **Activate Cortex Guard**: Enable the `cortex guardrails` parameter in your `AI_COMPLETE` calls. This uses *Llama Guard* to automatically filter out unsafe or non-compliant responses generated by the model.
2. **Apply RBAC Controls**: Restrict model access to specific roles using `USE ROLE`. Revoke access to the `PUBLIC` role for high-tier models to prevent unauthorized "trial-and-error" attacks.

### Phase 4: Monitoring & Governance (LLMOps)
Security is not a one-time setup; it requires continuous visibility.
1. **Log Everything**: Store every prompt and response in a secure Snowflake table. Use **Snowflake Trust Center** to monitor these logs for anomalies like sudden spikes in query complexity or unusual instruction patterns.
2. **Feedback & Evaluation**: Implement a feedback loop (e.g., using Streamlit) to have human experts or a "judge" LLM rate responses for safety and accuracy.
3. **Network Security**: Keep the entire inference process within the Snowflake secure boundary to ensure data in transit is encrypted with TLS 1.2+